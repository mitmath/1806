{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Identity Matrix\n",
    "\n",
    "A very important special matrix *identity* matrix $I$.   Here is a $ 5 \\times 5$ identity matrix:\n",
    "\n",
    "$$\n",
    "I = \\begin{pmatrix} 1&0&0&0&0 \\\\ 0&1&0&0&0 \\\\ 0&0&1&0&0 \\\\ 0&0&0&1&0 \\\\ 0&0&0&0&1 \\end{pmatrix}\n",
    "= \\begin{pmatrix} e_1 & e_2 & e_3 & e_4 & e_5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the columns $e_1 \\cdots e_5$ of $I$ are the **unit vectors** in each component.\n",
    "\n",
    "The identity matrix, which can be constructed by `eye(5)` in Julia, has the property that $Ix=x$ for any $x$, and hence $IA = A$ for any (here $5 \\times 5$) matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       "  4  -2  -7  -4  -8\n",
       "  9  -6  -6  -1  -5\n",
       " -2  -9   3  -5   2\n",
       "  9   7  -9   5  -8\n",
       " -1   6  -3   9   6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [4  -2  -7  -4  -8\n",
    "     9  -6  -6  -1  -5\n",
    "    -2  -9   3  -5   2\n",
    "     9   7  -9   5  -8\n",
    "    -1   6  -3   9   6] # a randomly chosen 5x5 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " -7\n",
       "  2\n",
       "  4\n",
       " -4\n",
       " -7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [-7,2,4,-4,-7] # a randomly chosen right-hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       " 1  0  0  0  0\n",
       " 0  1  0  0  0\n",
       " 0  0  1  0  0\n",
       " 0  0  0  1  0\n",
       " 0  0  0  0  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ = eye(Int, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ * b == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ * A == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * I₅ == A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notational ambiguity\n",
    "\n",
    "In principle, \"$I$\" is a little ambiguous in linear algebra, because there are $m \\times m$ identity matrices **of every size** $m$, and $I$ could refer to *any* of them.  In practice, you usually infer the size of $I$ **from context**: if you see an expression like $Ix$ or $IA$ or $A + I$, then $I$ refers to the identity matrix of the corresponding size.\n",
    "\n",
    "There is actually a built-in constant called `I` in Julia that represents just that: an identity matrix whose size is inferred from context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniformScaling{Int64}\n",
       "1*I"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " -7\n",
       "  2\n",
       "  4\n",
       " -4\n",
       " -7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I * A == A == A * I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I * [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is an ambiguity, I will sometimes write $I_m$ for the $m \\times m$ identity matrix.\n",
    "\n",
    "For example, if $B$ is an $3\\times2$ matrix, then\n",
    "\n",
    "$$\n",
    "I_3 B I_2 = B\n",
    "$$\n",
    "\n",
    "where we need to multiply by a different-size identity matrix on the left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Array{Int64,2}:\n",
       " 2  3\n",
       " 4  5\n",
       " 6  7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = [2 3\n",
    "     4 5\n",
    "     6 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Array{Int64,2}:\n",
       " 2  3\n",
       " 4  5\n",
       " 6  7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I * B * I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Array{Float64,2}:\n",
       " 2.0  3.0\n",
       " 4.0  5.0\n",
       " 6.0  7.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye(3) * B * eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"matrix A has dimensions (2,2), matrix B has dimensions (3,2)\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"matrix A has dimensions (2,2), matrix B has dimensions (3,2)\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m_generic_matmatmul!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Char, ::Char, ::Array{Float64,2}, ::Array{Int64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:492\u001b[22m\u001b[22m",
      " [2] \u001b[1mgeneric_matmatmul!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Char, ::Char, ::Array{Float64,2}, ::Array{Int64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:483\u001b[22m\u001b[22m",
      " [3] \u001b[1m*\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Array{Int64,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./operators.jl:424\u001b[22m\u001b[22m",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "eye(2) * B * eye(3)  # should give an error: wrong-shape matrices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Inverses\n",
    "\n",
    "It is often conceptually convenient to talk about the *inverse* $A^{-1}$ of a matrix $A$, which exists **for any non-singular square matrix**.   This is the matrix such that $x = A^{-1}b$ solves $Ax = b$ for any $b$.   The inverse is conceptually convenient becuase it allows us to move matrices around in equations *almost* like numbers (except that matrices don't commute!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, the inverse matrix $A^{-1}$ is the matrix such that $A^{-1} A = A A^{-1} = I$.\n",
    "\n",
    "Why does this correspond to solving $Ax=b$?  Multiplying both sides on the *left* by $A^{-1}$ (multiplying on the *right* would make no sense: we can't multiply vector×matrix!), we get \n",
    "\n",
    "$$\n",
    "A^{-1}Ax=Ix = x = A^{-1}b\n",
    "$$\n",
    "\n",
    "In Julia, you can just do `inv(A)` to get the inverse of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  0.0109991   0.529789  -0.908341  -0.635197  -0.0879927\n",
       "  0.131989    0.35747   -0.900092  -0.622365  -0.055912 \n",
       " -0.235564   -0.179652   0.370302   0.353804  -0.11549  \n",
       " -0.301558   -0.69172    1.48701    1.16499    0.0791323\n",
       "  0.2044      0.678582  -1.29667   -1.05408    0.0314696"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0          -6.66134e-16  -1.11022e-16  -1.11022e-16   2.22045e-16\n",
       "  8.32667e-17   1.0           2.498e-16     1.11022e-16  -4.996e-16  \n",
       "  0.0           8.88178e-16   1.0           4.44089e-16   0.0        \n",
       " -5.82867e-16  -5.55112e-17   2.77556e-17   1.0          -5.55112e-17\n",
       "  3.26128e-16  -1.66533e-16   9.71445e-16  -2.77556e-16   1.0        "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(A) * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is the identity matrix up to **roundoff errors**: computers only keep about 15 significant digits when they are doing most arithmetic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverses and products\n",
    "\n",
    "Inverses have a special relationship to matrix products:\n",
    "$$\n",
    "(AB)^{-1} = B^{-1} A^{-1}\n",
    "$$\n",
    "\n",
    "The reason for this is that we must have $(AB)^{-1} AB = I$, and it is easy to see that $B^{-1} A^{-1}$ does the trick.  Equivalently, $AB$ is the matrix that first multiplies by $B$ then by $A$; to invert this, we must *reverse the steps*: first multiply by the inverse of $A$ and then by the inverse of $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "   2.85829    2.71613    5.16998  -10.3065\n",
       " -53.2082    26.126     12.0725    51.5514\n",
       "  77.1062   -42.9249   -21.5875   -66.4452\n",
       " -43.733     23.4497     7.63216   41.986 "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = rand(4,4)\n",
    "D = rand(4,4)\n",
    "inv(C*D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "   2.85829    2.71613    5.16998  -10.3065\n",
       " -53.2082    26.126     12.0725    51.5514\n",
       "  77.1062   -42.9249   -21.5875   -66.4452\n",
       " -43.733     23.4497     7.63216   41.986 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(D)*inv(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Elimination, Inverses, and LU Factorization\n",
    "\n",
    "Recall from the last lecture that we were looking at the Gaussian-elimination steps to convert a matrix $A$ into **upper-triangular form** via a sequence of **row operations**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "1 & 1 & -1  \\\\\n",
    "3 & 11 & 6 \n",
    "\\end{pmatrix}}_A\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 2 & 3 \n",
    "\\end{pmatrix}}_{E_1 A}\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 0 & \\boxed{1} \n",
    "\\end{pmatrix}}_{E_2 E_1 A}\n",
    "$$\n",
    "\n",
    "Since row operations correspond to **multiplying by matrices on the *left***, constructed the \"elimination matrices\" $E_1$ and $E_2$ corresponding to each of these elimination steps (under the first and second pivots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 1  0  0\n",
       " 0  1  0\n",
       " 0  1  1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1 3  1\n",
    "     1 1 -1\n",
    "     3 11 6]\n",
    "E1 = [ 1 0 0\n",
    "      -1 1 0\n",
    "      -3 0 1]\n",
    "E2 = [1 0 0\n",
    "      0 1 0\n",
    "      0 1 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we arrived at the formula:\n",
    "$$\n",
    "\\underbrace{E_2 E_1}_E A = U\n",
    "$$\n",
    "Notice that we multiplied $A$ by the elimination matrices from *right to left* in the order of the steps: it is $E_2 E_1 A$, *not* $E_1 E_2 A$.  Because matrix multiplication is generally [not commutative](https://en.wikipedia.org/wiki/Commutative_property), $E_2 E_1$ and $E_1 E_2$ give *different* matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       "  1  0  0\n",
       " -1  1  0\n",
       " -4  1  1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2*E1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       "  1  0  0\n",
       " -1  1  0\n",
       " -3  1  1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E1*E2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, furthermore, that the matrices $E_1$ and $E_2$ are both **lower-triangular matrices**.  This is a consequence of the structure of Gaussian elimination (assuming no row re-ordering): we always add the pivot row to rows *below* it, never *above* it.\n",
    "\n",
    "The *product* of lower-triangular matrices is always lower-triangular too.  (In homework, you will explore a similar property for upper-triangular matrices)  In consequence, the product $E = E_2 E_1$ is lower-triangular, and Gaussian elimination can be viewed as yielding\n",
    "\n",
    "$$EA=U$$\n",
    "\n",
    "where $E$ is lower triangular and $U$ is upper triangular!\n",
    "\n",
    "# Inverse elimination: LU factors\n",
    "\n",
    "However, in practice, it turns out to be more useful to write this as\n",
    "\n",
    "$$A= E^{-1} U = E_1^{-1}E_2^{-1}U$$\n",
    "\n",
    "where $E^{-1}$ is the [inverse of the matrix](http://mathworld.wolfram.com/MatrixInverse.html) $E$.  We will have more to say about how to compute matrix inverses later in 18.06, but for now we just need to know that it is the matrix that **reverses the steps** of Gaussian elimination, taking us back from $U$ to $A$.  Computing matrix inverses is laborious in general, but in this particular case it is easy.   We just need to *reverse the steps one by one* starting with the *last* elimination step and working back to the *first* one.  \n",
    "\n",
    "Hence, just as for our general rule about inverses of products above, we need to reverse (invert) $E_2$ *first* on $U$, and *then* reverse (invert) $E_1$: $A = E_1^{-1} E_2^{-1} U$.  But reversing an individual elimination step like $E_2$ is easy: we just **flip the signs below the diagonal**, so that wherever we *added* the pivot row we *subtract* and vice-versa.  That is:\n",
    "$$\n",
    "\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}^{-1} =\n",
    "\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}\n",
    "$$\n",
    "(The last elimination step was adding the second row to the third row, so we reverse it by *subtracting* the second row from the third row of $U$.)\n",
    "\n",
    "Julia can compute matrix inverses for us with the `inv` function.  (It doesn't know the trick of flipping the sign, which only works for very special matrices, but it can compute it the \"hard way\" so quickly (for such a small matrix) that it doesn't matter.)   Of course that gives the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   0.0  0.0\n",
       " 0.0   1.0  0.0\n",
       " 0.0  -1.0  1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for $E_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0  0.0  0.0\n",
       " 1.0  1.0  0.0\n",
       " 3.0  0.0  1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(E1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't make any mistakes, then $E_1^{-1} E_2^{-1} U$ should give $A$, and it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   3.0   1.0\n",
       " 0.0  -2.0  -2.0\n",
       " 0.0   0.0   1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U = lu(A, Val{false})\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   3.0   1.0\n",
       " 1.0   1.0  -1.0\n",
       " 3.0  11.0   6.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(E1)*inv(E2)*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call *inverse* elimination matrix $L = E^{-1} = E_1^{-1} E_2^{-1}$  Since the inverses of each elimination matrix were lower-triangular (with flipped signs), their product $L$ is also lower triangular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   0.0  0.0\n",
       " 1.0   1.0  0.0\n",
       " 3.0  -1.0  1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = inv(E1)*inv(E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this is the same as the inverse of $E = E_2 E_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   0.0  0.0\n",
       " 1.0   1.0  0.0\n",
       " 3.0  -1.0  1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(E2*E1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result, therefore, is that Gaussian elimination (without row swaps) can be viewed as a *factorization* of the original matrix $A$\n",
    "$$\n",
    "A = LU\n",
    "$$\n",
    "into a **product of lower- and upper-triangular matrices**.  (Furthermore, although we didn't comment on this above, $L$ is always 1 along its diagonal.)  This factorization is called the [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition) of $A$.  (It's why we used the `lu` function in Julia above.)  When a computer performs Gaussian elimination, what it computes are the $L$ and $U$ factors.\n",
    "\n",
    "What this accomplishes is to break a complicated matrix $A$ into **much simpler pieces** $L$ and $U$.  It may not seem at first that $L$ and $U$ are *that* much simpler than $A$, but they are: lots of operations that are very difficult with $A$, like solving equations or computing the determinant, become *easy* once you known $L$ and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: Not for all triangular matrices\n",
    "\n",
    "Above, we used the trick that we can invert a *single* elimination matrix $E$ just by flipping the signs below the diagonal (to reverse the steps). **This doesn't work for a general lower (or upper) triangular matrix**.\n",
    "\n",
    "For example, even for the $L$ matrix computed above, it doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   0.0  0.0\n",
       " 1.0   1.0  0.0\n",
       " 3.0  -1.0  1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  1.0  0.0  0.0\n",
       " -1.0  1.0  0.0\n",
       " -4.0  1.0  1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 3 in the lower-left corner of $L$ changes to a -4 in the lower-left of $L^{-1}$!\n",
    "\n",
    "The reason it works for the \"elementary\" $E$ matrices representing elimination of a *single column* of $A$, but not for $L$ (or other lower-triangular matrices in general), is essentially that operations on different columns affect one another (the elimination steps don't commute).\n",
    "\n",
    "In fact, there is *no easy trick to invert a general lower/upper triangular matrix* quickly.  But you shouldn't have to!   If you see an expression like $L^{-1} b$, you shouldn't compute $L^{-1}$ explicitly!  Instead, solve $Lc = b$ by **forward-substitution.**\n",
    "\n",
    "Similarly, if you see $U^{-1} c$ for an upper-triangular matrix, don't try to compute $U^{-1}$. Instead, solve $Ux=c$ by **back-substitution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Matrix Inverses in General\n",
    "\n",
    "How do we find $A^{-1}$?  The key is the equation $A A^{-1} = I$, which looks just like $AX=B$ for the **right-hand sides consisting of the columns of the identity matrix**, i.e. the unit vectors.   So, we just solve $Ax=e_i$ for $i=1,\\ldots,5$, or equivalently do `A \\ I` in Julia.  Of course, Julia comes with a built-in function `inv(A)` for computing $A^{-1}$ as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       "  4  -2  -7  -4  -8\n",
       "  9  -6  -6  -1  -5\n",
       " -2  -9   3  -5   2\n",
       "  9   7  -9   5  -8\n",
       " -1   6  -3   9   6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [4  -2  -7  -4  -8\n",
    "     9  -6  -6  -1  -5\n",
    "    -2  -9   3  -5   2\n",
    "     9   7  -9   5  -8\n",
    "    -1   6  -3   9   6] # a randomly chosen 5x5 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  0.0109991   0.529789  -0.908341  -0.635197  -0.0879927\n",
       "  0.131989    0.35747   -0.900092  -0.622365  -0.055912 \n",
       " -0.235564   -0.179652   0.370302   0.353804  -0.11549  \n",
       " -0.301558   -0.69172    1.48701    1.16499    0.0791323\n",
       "  0.2044      0.678582  -1.29667   -1.05408    0.0314696"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv = A \\ I₅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.9984e-15    1.55431e-15  -2.88658e-15  -2.33147e-15   4.996e-16  \n",
       "  1.88738e-15   1.55431e-15  -3.21965e-15  -2.22045e-15   4.92661e-16\n",
       " -8.04912e-16  -1.55431e-15   1.88738e-15   1.11022e-15  -1.94289e-16\n",
       " -3.83027e-15  -3.88578e-15   5.9952e-15    5.55112e-15  -7.63278e-16\n",
       "  3.16414e-15   3.77476e-15  -5.32907e-15  -4.44089e-15   6.73073e-16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv - inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The difference is just roundoff errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0          -1.22125e-15  -1.31561e-14  -1.33227e-15  -7.43849e-15\n",
       "  6.69603e-15   1.0          -1.36557e-14   5.55112e-16  -9.10383e-15\n",
       " -1.09079e-14   1.33227e-15   1.0          -6.66134e-16   8.21565e-15\n",
       " -1.22541e-14   1.13243e-14   2.18714e-14   1.0           1.31006e-14\n",
       "  1.56403e-14  -8.57647e-15  -2.05808e-14  -5.77316e-15   1.0        "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again, we get $I$ up to roundoff errors because the computer does arithmetic only to 15–16 significant digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0           0.0           0.0           1.77636e-15  -5.55112e-17\n",
       " -4.44089e-16   1.0           0.0           8.88178e-16   2.22045e-16\n",
       "  1.72085e-15   5.32907e-15   1.0          -8.43769e-15  -7.49401e-16\n",
       " -5.32907e-15  -8.88178e-15   5.32907e-15   1.0           5.55112e-16\n",
       " -3.10862e-15   0.0          -1.77636e-15   7.10543e-15   1.0        "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * Ainv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, $AB \\ne BA$ for two matrices $A$ and $B$.  Why can we multiply $A$ by $A^{-1}$ on either the left or right and get the same answer $I$?  It is fairly easy to see why:\n",
    "\n",
    "$$\n",
    "A A^{-1} = I \\implies A A^{-1} A = IA = A = A (A^{-1} A)\n",
    "$$\n",
    "\n",
    "Since $A (A^{-1} A) = A$, and $A$ is non-singular (so there is a unique solution to this system of equations), we must have $A^{-1} A = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       "  0.505958   0.505958\n",
       " -0.928506  -0.928506\n",
       "  2.16407    2.16407 \n",
       "  1.46166    1.46166 \n",
       " -1.26428   -1.26428 "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[A\\b Ainv*b] # print the two results side-by-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Almost) Never Compute Inverses!\n",
    "\n",
    "Matrix inverses are funny, however:\n",
    "\n",
    "* Inverse matrices are very convenient in *analytical* manipulations, because they allow you to move matrices from one side to the other of equations easily.\n",
    "\n",
    "* Inverse matrices are **almost never computed** in \"serious\" numerical calculations.  Whenever you see $A^{-1} B$ (or $A^{-1} b$), when you go to *implement* it on a computer you should *read* $A^{-1} B$ as \"solve $AX = B$ by some method.\" e.g. solve it by `A \\ B` or by first computing the LU factorization of $A$ and then using it to solve $AX = B$.\n",
    "\n",
    "One reason that you don't usually compute inverse matrices is that it is wasteful: once you have $A=LU$ (later we will generalize this to \"$PA = LU$\"), you can solve $AX=B$ directly without bothering to find $A^{-1}$, and computing $A^{-1}$ requires much more work if you only have to solve a few right-hand sides.\n",
    "\n",
    "Another reason is that for many special matrices, there are ways to solve $AX=B$ *much* more quickly than you can find $A^{-1}$.   For example, many large matrices in practice are [sparse](https://en.wikipedia.org/wiki/Sparse_matrix) (mostly zero), and often for sparse matrices you can arrange for $L$ and $U$ to be sparse too.  Sparse matrices are much more efficient to work with than general \"dense\" matrices because you don't have to multiply (or even store) the zeros. Even if $A$ is sparse, however, $A^{-1}$ is usually non-sparse, so you lose the special efficiency of sparsity if you compute the inverse matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
