{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam 3 practice problems - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $E_n$ by the symmetric tridiagonal ones matrix.  For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5√ó5 Tridiagonal{Int64,Array{Int64,1}}:\n",
       " 1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " 1  1  1  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  1  1  1  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  1  1  1\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "E(n) = Tridiagonal(ones(Int,n-1),ones(Int,n),ones(Int,n-1))\n",
    "\n",
    "E(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1a) When n=5, column 1 plus column 4 is the \"ones\" vector.  Find two other columns that add to the ones vector.\n",
    "What is the determinant of $E_5$? and provide one  eigenvalue and corresponding eigenvector of $E_5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1b) Is $E_5$ diagonalizible?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1c) Is the cofactor matrix of $E_n$ symmetric for all n?  Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6√ó6 Tridiagonal{Int64,Array{Int64,1}}:\n",
       " 1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " 1  1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  1  1  1  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  1  1  1  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1  1\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1d) What is the determinant of $E_6$? What is the volume of the image of the unit cube: <br>\n",
    "    $\\{ E_6x, x\\in R^6,  0 \\le x_i \\le 1 \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8√ó8 Tridiagonal{Int64,Array{Int64,1}}:\n",
       " 1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " 1  1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  1  1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  1  1  1  ‚ãÖ  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1  1  ‚ãÖ  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1  1  ‚ãÖ\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1  1\n",
       " ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  ‚ãÖ  1  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1e) It turns out that for $n=2,5,8,11,14$ $E_n$ is singular.  Find an eigenvector of $E_n$ for all such $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1f) What is the sum of the eigenvalues of $E_n$ for all n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n=7, we can compute $E_n^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7√ó7 Array{Int64,2}:\n",
       " 2  2  1  0  0  0  0\n",
       " 2  3  2  1  0  0  0\n",
       " 1  2  3  2  1  0  0\n",
       " 0  1  2  3  2  1  0\n",
       " 0  0  1  2  3  2  1\n",
       " 0  0  0  1  2  3  2\n",
       " 0  0  0  0  1  2  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E(7)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1g) What is the sum of the squares of the eigenvalues of $E_8$? Generalize for all n. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7√ó7 Array{Float64,2}:\n",
       "  1.0   0.0  -1.0   1.0   0.0  -1.0   1.0\n",
       "  0.0   0.0   1.0  -1.0   0.0   1.0  -1.0\n",
       " -1.0   1.0   0.0   0.0   0.0   0.0   0.0\n",
       "  1.0  -1.0   0.0   1.0   0.0  -1.0   1.0\n",
       "  0.0   0.0   0.0   0.0   0.0   1.0  -1.0\n",
       " -1.0   1.0   0.0  -1.0   1.0   0.0   0.0\n",
       "  1.0  -1.0   0.0   1.0  -1.0   0.0   1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(E(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1h) Use the above computation to say what is the sum of the reciprocals of the eigenvalues of $E_7$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(1a) Column 1 plus column 4 gives the ones vector. Column 2 plus column 5 also gives the ones vector. Let $v_i$ be the $i$-th column of $E_5$. Then\n",
    "$$v_1+v_4 = v_2+v_5$$\n",
    "\n",
    "This means that there is a linear dependence between the columns of $E_5$. This in turn means that the rank of $E_5$ must be less than 5. $E_5$ is therefore a singular matrix and so $\\det E_5 = 0$.\n",
    "\n",
    "If $E_5$ is singular then it must have at least one zero eigenvalue $\\lambda = 0$. The corresponding eigenvector will be in the nullspace of $E_5$. Using the linear dependence between the columns, we can deduce that an eigenvector for the zero eigenvalue will be\n",
    "$$\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$$\n",
    "\n",
    "(1b) $E_5$ is a symmetric matrix. We learnt in class that every symmetric matrix is diagonalizable. Therefore $E_5$ is diagonalizable.\n",
    "\n",
    "(1c) The cofactor matrix $C$ of a matrix $A$ has components\n",
    "$$C_{ij} = (-1)^{i+j} \\det M_{ij}$$\n",
    "Where $M_{ij}$ is the matrix obtained by removing the $i$-th row and $j$-th column from $A$. if $A$ is a symmetric matrix, then $M_{ij} = M_{ji}^T$ (try this out for a few simple cases). This means that\n",
    "\\begin{align}\n",
    "C_{ji} &= (-1)^{j+i} \\det M_{ji}\\\\\n",
    "       &= (-1)^{i+j} \\det M_{ij}^T \\\\\n",
    "       &=  (-1)^{i+j} \\det M_{ij} = C_{ij}\n",
    "\\end{align}\n",
    "and so the cofactor matrix of any symmetric matrix is also symmetric.\n",
    "\n",
    "(1d) We can calculate the determinant of $E_6$ using the cofactor expansion\n",
    "\\begin{align}\n",
    "\\det E_6 &= \\begin{vmatrix} 1 & 1 & & & &\\\\1 & 1 & 1 & & &\\\\ & 1 & 1 & 1 & & \\\\& & 1 & 1 & 1 & \\\\ & & & 1 & 1 & 1 \\\\ & & & & 1 & 1 \\end{vmatrix}\\\\\n",
    "&= \\begin{vmatrix}  1 & 1 & & &\\\\ 1 & 1 & 1 & & \\\\ & 1 & 1 & 1 & \\\\  & & 1 & 1 & 1 \\\\  & & & 1 & 1 \\end{vmatrix}        - \\begin{vmatrix} 1  & 1 & & &\\\\  & 1 & 1 & & \\\\&  1 & 1 & 1 & \\\\ &  & 1 & 1 & 1 \\\\ &  & & 1 & 1 \\end{vmatrix}\\\\\n",
    "&= \\det E_5 + \\begin{vmatrix}   & 1 & & \\\\ & 1 & 1 & \\\\ &  1 & 1 & 1 \\\\ &  & 1 & 1 \\end{vmatrix} - \\begin{vmatrix}    1 & 1 & & \\\\  1 & 1 & 1 & \\\\   & 1 & 1 & 1 \\\\   & & 1 & 1 \\end{vmatrix}\\\\\n",
    "&= - \\begin{vmatrix}    1 & 1 & \\\\  1 & 1 & 1 \\\\  & 1 & 1 \\end{vmatrix} + \\begin{vmatrix}     1  & 1 & \\\\    & 1 & 1 \\\\   &  1 & 1 \\end{vmatrix}\\\\\n",
    "&= - \\begin{vmatrix} 1 & 1 \\\\  1 & 1 \\end{vmatrix} + \\begin{vmatrix}   1  & 1 \\\\   & 1 \\end{vmatrix}\\\\\n",
    "&= 1\n",
    "\\end{align}\n",
    "\n",
    "This means that $E_6$ is volume preserving, and so the volume of the image of the unit cube will be the same as the volume of the original cube, i.e. 1.\n",
    "\n",
    "(1e) If $E_n$ is singular for $n=2,5,8,11,14$, then each of these matrices will have a zero eigenvalue $\\lambda = 0$. The corresponding eigenvector will be in the nullspace of each matrix:\n",
    "\n",
    "* For $n=2$, we have \n",
    "$$E_2 = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$$\n",
    "We can immediately identify that \n",
    "$$v = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n",
    "is in the nullspace\n",
    "\n",
    "* For $n=5$, we already found that\n",
    "$$ v = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$$ \n",
    "is an eigenvector for $\\lambda = 0$\n",
    "\n",
    "* For $n=8$, we notice that the sum of columns 1, 4 and 7 is the ones vector. The sum of columns 2, 5 and 8 is also the ones vector. This means that\n",
    "$$ v = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$$ \n",
    "\n",
    "* We can now start to spot a pattern. Any matrix $E_n$ where $n = 3k+2$ for an integer $k$ will be singular. For $n=11$ an eigenvector for the zero eigenvalue is $ v = ( 1 , -1 , 0 , 1 , -1 , 0 , 1 , -1 , 0 , 1 , -1)^T$ \n",
    "For $n=14$ an eigenvector for the zero eigenvalue is $ v = ( 1 , -1 , 0 , 1 , -1 , 0 , 1 , -1 , 0 , 1 , -1 , 0 , 1 , -1)^T$\n",
    "\n",
    "(1f) The sum of the eigenvalues of a matrix is always equal to the trace of the matrix. The trace of $E_n$ is $n$, and so\n",
    "$$\\boxed{\\sum_{i=1}^n \\lambda_i = n}$$\n",
    "\n",
    "(1g) The eigenvalues of the square of a matrix are the squares of the eigenvalues of the original matrix. This means that the sum of the squares of the eigenvalues of $E_8$ is equal to the trace of $E_8^2$. The pattern along the diagonal of $E_8$ will be the same as for $E_7$ - the entries in the corners will both be 2, while every other element on the diagonal will be 3. We can therefore deduce that\n",
    "$$\\boxed{\\sum_{i=1}^8 \\lambda_i^2 = 6\\times 3 + 2 + 2 = 22}$$\n",
    "\n",
    "(1h) The eigenvalues of the inverse of a matrix are the reciprocals of the eigenvalues of the original matrix. THe sum of the reciprocals of the eigenvalues of $E_7$ will therefore be given by the trace of $E_7^{-1}$. Hence\n",
    "$$\\boxed{\\sum_{i=1}^7 \\lambda_i^{-1} = 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "(2a) What is the determinant of $A = \\left(\\begin{array}{rrrrr}1&0&0&0&0\\\\- a&1&0&0&0\\\\0&- b&1&0&0\\\\0&0&- c&1&0\\\\0&0&0&- d&1\\end{array}\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2b) What are the eigenvalues of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2c) Give an example (by picking a,b,c,d) where $A$ is diagonalizible, and another example where it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2d) What is the first column of the inverse of $A$ using determinants and cofactors? (This question should not require much pencil and paper work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(2a) $A$ is a triangular matrix. The determinant of $A$ is therefore the product of its diagonal entries. Hence\n",
    "$$\\boxed{\\det A = 1}$$\n",
    "\n",
    "(2b) The eigenvalues of a triangular matrix are the entries along the diagonal. Since the entries along the diagonal of $A$ are all 1, the eigenvalues of $A$ are all equal to 1.\n",
    "\n",
    "(2c) When $a=b=c=d=0$, $A=I$ which is diagonal and therefore certainly diagonalisable. However, if we set $a=b=c=0$, but $d=-1$, then $A$ only has four independent eigenvectors (try writing out the equations and you'll soon see there are only four possible independent eigenvectors)\n",
    "\n",
    "(2d) In order to find the first colun of $A^{-1}$, we recall that\n",
    "$$A^{-1} = \\frac{1}{\\det A} C^T,$$\n",
    "where $C$ is the matrix of cofactors. We therefore need the cofactors of the form $C_{1j}$. These can be calculated (this does not require much work!) to be\n",
    "\\begin{align}\n",
    "C_{11} &= 1\\\\\n",
    "C_{12} &= a\\\\\n",
    "C_{13} &= ab \\\\\n",
    "C_{14} &= abc \\\\\n",
    "C_{15} & = abcd\n",
    "\\end{align}\n",
    "And so the first column of $A^{-1}$ is given by the vector\n",
    "$$\\begin{pmatrix} 1 \\\\ a \\\\ ab \\\\ abc \\\\ abcd \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(3) Let P be the permutation matrix\n",
    "$$ \\begin{pmatrix} . &1 & . & .\\\\ . & . & 1 & . \\\\ . & . &  . & 1 \\\\ 1 & . & . & .\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3a) What is $P^4$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3b) It turns out $P$ has four distinct eigenvalues.  Use your answer to (3a) to say what they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3c) Does $P$ have four distinct singular values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3d) Find an eigenvector of $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3e) Compute det(P$\\pm$I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "(3a) $P^4 = I$\n",
    "\n",
    "(3b) If $P$ has four distinct eigenvalues, then they are the four distinct numbers for which $\\lambda^4 = 1$ (Since the identity matrix only has the eigenvalue 1). These four numbers are\n",
    "$$\\boxed{\\lambda = \\pm 1, \\pm i}$$\n",
    "\n",
    "(3c) The singular values of $P$ are all equal to $1$. This is the case for any orthogonal matrix.\n",
    "\n",
    "(3d) The vector $v = (1,1,1,1)^T$ is an eigenvector of $P$ with eigenvalue $\\lambda = 1$, $v = (1,-1,1,-1)^T$ for $\\lambda = -1$, $v= (-i,1,i,-1)^T$ for $\\lambda = i$, and $v= (i,1,-i,-1)^T$ for $\\lambda = -i$\n",
    "\n",
    "(3e) $\\det (P-\\lambda I) = 0$, whenever $\\lambda$ is an eigenvalue of $P$. Since $\\lambda = \\pm 1$ are eigenvalues of $P$, $\\det (P \\pm I ) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "(4) A matrix can be factored as $A=X\\Lambda X^{-1}$, where $\\Lambda$ is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4a) This matrix is i)definitely, ii)possibly, iii)definitely not invertible. (pick one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4b) This matrix i)definitely ii)possible, iii) definitely not diagonalizible (pick one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4c) Find an eigen factorization for $(A-I)(A-2I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4d) Find a factorization for  $(ùê¥‚àí\\lambda_1 I)\\ldots(ùê¥‚àí\\lambda_nùêº)$ and then put this matrix in simplest form ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4e) Suppose the largest eigenvalue has absolute value less than 1.  Then the matrix\n",
    "$A-I$ is i) definitely ii) possibly iii)definitely not invetible. (pick one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4f) Suppose the largest eigenvalue has absolute value exactly 1.  Then the matrix\n",
    "$A-I$ is i) definitely ii) possibly iii)definitely not invetible. (pick one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(4a) This matrix is possibly invertible. \n",
    "\n",
    "(4b) This matrix is definitely diagonalizable.\n",
    "\n",
    "(4c) Suppose that $v$ is an eigenvector of $A$ with eigenvalue $\\lambda$. Then $Av = \\lambda v$. We can then see that $v$ is an eigenvector of $(A-I)(A-2I)$ as follows:\n",
    "$$ (A-I)(A-2I)v = (A-I)(Av - 2v) = (A-I)(\\lambda - 2)v = (\\lambda-1)(\\lambda - 2)v.$$\n",
    "Therefore $A$ and $(A-I)(A-2I)$ have the same eigenvectors, and the eigenvectors are related by $\\lambda \\to (\\lambda-1)(\\lambda - 2)$, so that\n",
    "$$ (A-I)(A-2I) = X((\\Lambda-I)(\\Lambda-2I))X^{-1}$$\n",
    "\n",
    "(4d) $(ùê¥‚àí\\lambda_1 I)\\ldots(ùê¥‚àí\\lambda_n I)$ has the same eigenvectors as $A$ via the same argument as part (c). However, the corresponding eigenvalues will now all be 0. This means that\n",
    "$$(ùê¥‚àí\\lambda_1 I)\\ldots(ùê¥‚àí\\lambda_nùêº) = X\\mathbf{0}X^{-1} = \\mathbf{0}$$\n",
    "where $\\mathbf{0}$ is the zero matrix.\n",
    "\n",
    "(4e) If the largest eigenvalue of $A$ has magnitude less than 1 then $\\lambda = 1$ cannot be an eigenvalue. Therefore $\\det (A-I) \\neq =0$ and so $A-I$ is definitely invertible.\n",
    "\n",
    "(4f) If the largest eigenvalue of $A$ has absolute value exactly 1, then $A-I$ is possibly invertible. For example, if $A = -I$, then all of the eigenvalues of $A$ have absolute value equal to 1, but $-2I$ is certainly invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "(5) The image of the unit cicle for a certain 2x2 symmetric matrix is an ellipse with semiaxes\n",
    " 1806 and 2019. What are all the possibilities for the eigenvalues of this matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "This means the singular values are 1806 and 2019 which are the absolute values of the real eigenvalues.\n",
    "Thus the eigenvalues can be $\\pm 1806$ and $\\pm 2019$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "(6) True or false.  A matrix with real eigenvalues and eigenvectors must be symmetric.  Explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "This is false. For example, any upper triangular real matrix will have real eigenvalues (the entries along its diagonal), but will certainly not be symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "(7) A matrix is symmetric and orthogonal.  What are the possible eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "Suppose the matrix $A$ is orthogonal and symmetric. Then $A^TA=I$ and $A=A^T$. This means that \n",
    "$$A^TA = A^2 = I$$\n",
    "The eigenvalues of the identity matrix are all equal to 1. But since $A^2=I$, this must mean that the possible eigenvalues of $A$ are the square roots of 1, i.e.\n",
    "$$\\boxed{\\lambda = \\pm 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "(8) A matrix is a projection matrix and orthogonal.  What are the possible matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "A projection matrix is symmetric and satisfies $P^2=P=P^T$. If it is orthogonal $P^TP=I$.\n",
    "Combining these we see that $P=I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "(9) A matrix has all of its eigenvalues $0$.  The set of matrices similar is i)infinite ii)possibly finite iii)must be finite.  (Pick the best answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "The answer is possibly finite. The zero matrix has all of its eigenvalues $0$. However the only matrix similar to the zero matrix is the zero matrix itself. To show this, suppose that $A$ is a matrix similar to the zero matrix, so that $0=XAX^{-1}$. However, we can invert this equation to yield that $A=X^{-1}0X = 0$, and so $A=0$. \n",
    "\n",
    "On the contrary, there are infinitely many matrices of the form\n",
    "$$\\begin{pmatrix} 0 & a \\\\ 0 & 0 \\end{pmatrix}$$\n",
    "for each $a\\in\\mathbb{R}$. Each of these matrices has two 0 eigenvalues. These matrices are all similar to the matrix \n",
    "$$\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},$$\n",
    "since we can write\n",
    "$$\\begin{pmatrix} 0 & a \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} a & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} 1/a & 0 \\\\ 0 & 1 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "(10) True or false.  The absolute value of the eigenvalues of a matrix $A$ always describe the semi-axes of\n",
    "the image of the unit ball under $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "This is false. Only the product of the absolute value of the eigenvalues must equal the product of the semi-axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "Suppose that all the nonzero eigenvalues of $A$ are distinct, and the dimension of the nullspace of $A$ is equal to the number of $0$ eigenvalues.  Why is $A$ diagonalizible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "Vectors in the nullspace are also eigenvectors corresponding to $\\lambda = 0$. Suppose $A$ is $n\\times n$. with $m$ $0$ eigenvalues. There are then $m$ distinct eigenvectors for this eigenvalue. Since the remaining $n-m$ eigenvalues are distinct, there will be another $n-m$ distinct eigenvectors. So $A$ necessarily has $n$ distinct eigenvectors and so $A$ is diagonalisable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "(12) A matrix has all eigenvalues real and negative.  What can you say about the matrix exp(At) as $t\\rightarrow \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "Suppose $A$ has an eigenvalue $\\lambda$ with eigenvector $v$. Then $Av = \\lambda v$ and $exp(At)v = exp(\\lambda t)v$, and so $exp(At)$ has the same eigenvectors as $A$ with eigenvalues $exp(\\lambda t)$. If all of the $\\lambda$ are real and negative, then all of the eigenvalues of $exp(At)$ go to $0$ as $t\\to \\infty$. If $A$ has distinct eigenvectors, then we can say that $exp(At)$ tends to the zero matrix as $t\\to\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
