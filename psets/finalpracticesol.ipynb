{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to practice problems for 18.06 final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Give a brief and convincing argument.  (Not an example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1a) The square of a symmetric matrix is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1b) The square of a projection matrix is a projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1c) The square of a matrix with positive entries has positive entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1d) The square of a positive definite matrix is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1e) The square of a positive Markov matrix is a positive Markov matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1f) The square of an orthogonal matrix is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(1a) If $A$ is symmetric then $A=A^T$. We also recall the rule that $(AB)^T = B^TA^T$. So \n",
    "$$(A^2)^T = (AA)^T = A^TA^T = A^2,$$\n",
    "and so $A^2$ is also symmetric.\n",
    "\n",
    "(1b) A matrix $P$ is a projection matrix if it is symmetric and $P^2=P$. We showed in part (a) that the square of a symmetric matrix is also symmetric. We also need to show that $(P^2)^2 = P^2$. But since $P$ is a projection, both sides of this equality are $P$. So $P^2$ is also a projection matrix.\n",
    "\n",
    "(1c) Suppose every entry of a matrix $A$ is positive. Every entry of $A^2$ is obtained by taking the dot product of a column and row of $A$. Since every entry of $A$ is positive, each entry of $A^2$ is obtained by taking sums and products of positive numbers. And so every entry of $A^2$ will be positive. \n",
    "\n",
    "(1d) Suppose $A$ is positive definite. Then all of the eigenvalues $\\lambda$ of $A$ are real and positive. The eigenvalues of $A^2$ are $\\lambda^2$ which are also necessarily positive. And so $A^2$ is positive definite.\n",
    "\n",
    "(1e) A matrix $A$ is a positive Markov matrix precisely when all its entries are positive and $\\mathbf{1} A=\\mathbf{1}$, where $\\mathbf{1}$ denotes the all ones row vector of the appropriate size. We have shown that the square of a matrix with only positive entries has only positive entries. As $\\mathbf{1} A^2=(\\mathbf{1} A) A=\\mathbf{1} A=\\mathbf{1}$, we conclude that the square of a positive  Markov matrix is a positive Markov matrix.\n",
    "\n",
    "(1f) An orthogonal matrix obeys $Q^TQ = I$. Now \n",
    "$$(Q^2)^TQ^2 = Q^TQ^TQQ = Q^T(Q^TQ)Q = Q^TQ = I,$$\n",
    "and so $Q^2$ is also an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Find a $2\\times 2$ matrix that is Markov and Positive Definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The symmetric matrix  $$A = \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix}$$ has eigenvalues $1$ and $1-2p$.  For $0 \\le p<1/2$ the two eigenvalues are positive and the Matrix is Markov. For example take $p=1/4$, or even $p=0$ to get the identity (which is Markov but not positive Markov.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Am I a vector space? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3a) All nxn matrices with determinant 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3b) Given a fixed $n\\times n$ matrix $A$, the set of linear tranformations of the form $f(X) = AX + XA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3c)  The set consisting of nxn matrices A that are either positive definite, or -A is positive definite, or A is the zero matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3d) The set of nxn matrices of the form $A-A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3e)  Matrix Polynomials of the form $P(X) = A + BX + CX^2$, where $X$ is an indeterminate (symbolic) $n\\times n$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(a) This is **not** a vector space. If $A$ is a matrix with $\\det A = 1$, then $cA$ has determinant $c^n\\det A = c^n \\neq 1$ generally.\n",
    "\n",
    "(b) This is a vector space. Consider two such transformations in this set, $f_1(X) = AX+XA$ and $f_2(X) = BX + XB$. Consider an arbitrary linear combination of these two transformations $g(X) = af_1(X) + bf_2(X)$, where $a,b\\in\\mathbb{R}$. Then\n",
    "$$g(X) = a(AX+XA)+b(BX+XB) = (aA+bB)X+X(aA+bB),$$\n",
    "which is a linear transformation of the correct form.\n",
    "\n",
    "(c) This is **not** a vector space. Consider the two matrices $A=\\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$ and $B = \\begin{pmatrix} -2 & 0 \\\\ 0 & -1 \\end{pmatrix}$. Then $A$ and $-B$ have eigenvalues 1,2, so they belong to this set. However $A+B = \\begin{pmatrix} -1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ which has eigenvalues $\\pm 1$ and so does not belong to this set.\n",
    "\n",
    "(d) This is a vector space. Let $C = A-A^T$ and $D = B-B^T$ be two matrices in this set. Consider an arbitrary linear combination, where $\\lambda,\\mu \\in \\mathbb{R}$:\n",
    "$$\\lambda C + \\mu D = \\lambda (A-A^T)+\\mu(B-B^T) = (\\lambda A + \\mu B) - (\\lambda A^T + \\mu B^T) = (\\lambda A + \\mu B) - (\\lambda A + \\mu B)^T$$\n",
    "which is a matrix of the required form.\n",
    "\n",
    "(e) This is a vector space. Take two such matrix polynomials $P_1(X) = A+BX+CX^2$ and $P_2(X) = D+EX+FX^2$, and form a linear combination $P(X) = a P_1(X) + bP_2(X)$ where $a,b\\in\\mathbb{R}$, then\n",
    "$$P(X) = a(A+BX+CX^2) + b (D+EX+FX^2) = (aA+bD) + (aB+bE)X+(aC+bF)X^2,$$\n",
    "which is a matrix polynomial of the required form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Suppose $Q$ is an orthogonal eigenvector matrix of some symmetric matrix $S$, and $x$ is the sum of the columns of $Q$. What is $x^TSx$ in terms of the eigenvalues of $S$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $e$ be the \"ones vector\", so we have$x=Qe$.  Using the eigenfactorization $S=Q\\Lambda Q^T$, we see that\n",
    "$x^T S x=e^T \\Lambda e$ which is the sum of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "Set up a matrix least squares problem if we are interested in taking $n$ data points $(x_i,y_i)$ and we wish to find the best function $f(x)=c_1 \\sin(x) + c_2 \\cos(x)$ through the data points.\n",
    "Write the solution to the least squares problem in terms of the compact SVD of your matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let \n",
    "$$A = \\begin{pmatrix} \\sin{x_1} & \\cos{x_1} \\\\ \\sin{x_2} & \\cos{x_2} \\\\ \\vdots & \\vdots \\\\ \\sin{x_n} & \\cos{x_n} \\end{pmatrix}, \\;\\; b = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\;\\; x = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$$\n",
    "\n",
    "We want to then find $\\hat{x}$ which is the least squares solution to $Ax = b$, in the sense that it minimizes the least squares error $\\Vert A\\hat{x} -b \\Vert^2$. Recall that if $A=U\\Sigma V^T$ is the compact SVD of a matrix $A$, then the least squares solution is given by \n",
    "$$\\boxed{\\hat{x} = V\\Sigma^{-1} U^Tb}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "Find a basis for the following vector spaces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6a) Polynomials of degree at  most 4 whose 2nd and 3rd derivatives at 0 are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6b) 2x2 matrices for which trace($A^TX$)=0 where $A$ is the $2\\times 2$ \"ones\" matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(a) Consider the most general polynomial of degree at most 4 \n",
    "$$p(x) = ax^4+bx^3+cx^2+dx+e$$\n",
    "Then\n",
    "\\begin{align*}\n",
    "p''(x) &= 12ax^2+6bx+2c\\\\\n",
    "p'''(x) &= 24ax+6b\n",
    "\\end{align*}\n",
    "We can then see that the second and the third derivatives vanish at zero if and only $b=c=0$. So a basis for this vector spaces would be the set\n",
    "$$\\boxed{\\{x^4,x,1\\}}$$\n",
    "\n",
    "(b) Let $X=\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, then trace$(A^TX) = a+b+c+d$. The space of matrices for which trace$(A^TX) = 0 $ therefore depends on three parameters, e.g. we can choose any values for $a,b,c$ independently, but the value of $d$ will be fixed by this restriction. This vector space has dimension three, with a possible basis\n",
    "$$\\boxed{\\left\\{ \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \\begin{pmatrix} 0 & 1 \\\\ 0 & -1 \\end{pmatrix}, \\begin{pmatrix} 0 & 0 \\\\ 1 & -1 \\end{pmatrix}\\right\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "One definition of a symmetric matrix $S$ is that $S_{ij}=S_{ji}$ for all pairs $(i,j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six permutations of three indices $ijk$: $ijk$,$ikj$,$jik$, etc.  Find a basis for all symmetric $2\\times 2 \\times 2$ arrays, where symmetric is defined such that $S_{ijk} =  S_{ikj} = S_{jik} = ... $ (Six items all equal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "For $2\\times 2 \\times 2$ arrays, symmetry imposes the restriction that $S_{121} = S_{112} = S_{211}$ and $S_{122}=S_{212}=S_{221}$, while $S_{111}$ and $S_{222}$ are arbitrary. Symmetric $2\\times 2 \\times 2$ arrays therefore depend on four independent parameters, and so a basis will contain four independent arrays obeying the symmetry condition. A possible basis contains the four arrays with the following:\n",
    "* $S_{111} = 1$, all other entries 0\n",
    "* $S_{222} = 1$, all other entries 0\n",
    "* $S_{121} = S_{112} = S_{211} =1$, all other entries 0\n",
    "* $S_{122}=S_{212}=S_{221} =1$, all other entries 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n",
    "\n",
    "What is the dimension of the vector space of linear transformations from $2\\times 2\\times 2$ arrays to polynomials of degree 9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$2\\times 2\\times 2$ arrays form a $2^3 = 8$ dimensional vector space, while polynomials of degree at most 9 form a 10 dimensional vector space. In order to specify a linear transformation between these two vector spaces, we have to specify which polynomial of degree 9 we will send each of the eight elements in any basis of $2\\times 2\\times 2$ arrays. We will therefore need $8\\times 10 = 80$ parameters to specify such a linear transformation, so the vector space of these linear transformations is an $80$ dimensional vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n",
    "\n",
    "Let $x$ be a vector from $R^n$.  The vandermonde matrix $V$ is defined as\n",
    "$V_{ij} = x_i^{j-1}$. \n",
    "\n",
    "(9a) Is the set of $2019\\times 2019$ vandermonde matrices a vector space?   <br>\n",
    "\n",
    "(9b) How many parameters are needed to represent an $n\\times n$  vandermonde matrix on a computer? <br>\n",
    "\n",
    "(9c) Suppose $V$ is an invertible  vandermonde for a vector $x$ in $\\mathbb{R}^n$.  Let $y$ be a vector in $\\mathbb{R}^n$. The vector $c$ that satisfies $Vc=y$ are the coefficients of some polynomial.  This polynomial goes through which $n$ points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9d) Can a 2x2 vandermonde be positive Markov?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9e) Can a  2x2 vandermonde be positive definite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "(a) This is not a vector space. The first column of a Vandermonde matrix is a column of ones by definition. Therefore if we have a Vandermonde matrix and multiply by any constant that is not 1, we will obtain a matrix that cannot be a Vandermonde matrix.\n",
    "\n",
    "(b) In order to represent a Vandermonde matrix, we must know the components of the vector $x\\in\\mathbb{R}^n$. We will therefore need $n$ parameters (the components of $x$).\n",
    "\n",
    "(c) The equation $Vc=y$ is a system of $n$ equations of the form\n",
    "$$c_1+c_2x_i+c_3x_i^2+ ... + c_nx_i^{n-1} = y_i$$\n",
    "for $i=1,..,n$. Therefore the components of $c$ are the coefficients of a polynomial $y = c_1+c_2x+...+c_nx^{n-1}$ which passes through each of the $n$ points $(x_i,y_i)$.\n",
    "\n",
    "(d) This is not possible. The first column of a Vandermonde matrix is a column of ones, so this violates the requirement that the columns of a Markov matrix sum to 1.\n",
    "\n",
    "(e) Yes, this is possible. For example\n",
    "$$V = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10\n",
    "\n",
    "Compute the gradient of  $f(x)=x^ùëáx+w^Tx$  without the use of indices, where $x$ and $w$ are in $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Solution 1: We recall the technique is to identify $df$ as $dx^T(gradient)=(gradient)^Tdx$. <br>\n",
    "$$df = d(x^Tx+w^Tx)=dx^Tx + x^T dx +w^Tdx = dx^T(2x+w).$$\n",
    "So the gradient is $2x+w$.\n",
    "\n",
    "Solution 2: We can expand $f(x+dx)$ as follows:\n",
    "\\begin{align}\n",
    "f(x+dx) &= (x+dx)^T(x+dx) + w^T(x+dx)\\\\\n",
    "        &= x^Tx+(dx)^Tx+x^Tdx + (dx)^Tdx + w^Tx + w^Tdx\\\\\n",
    "        &= x^Tx + 2x^T dx + (dx)^Tdx + w^Tx + w^Tdx \\\\\n",
    "        &= x^Tx + w^Tx + (2x^T+w^T)dx + (dx)^Tdx \\\\\n",
    "        &= f(x) + (\\nabla f)^T dx + (dx)^Tdx \n",
    "\\end{align}\n",
    "and so \n",
    "$$\\boxed{\\nabla f = 2x+w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11\n",
    "\n",
    "Compute the gradient of $f(A)=  trace(A^T A) + trace(W^T A)$, where the matrices are $n \\times n$. The answer should be in the form of an $n\\times n$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "Solution 1: We recall that the technique is to identify $df$ as $trace(dA^T(gradient))=trace((gradient)^TdA)$. <br>\n",
    "$$df = trace(dA^T A) + trace(A^T dA) + trace(W^T dA) = trace(dA^T(2A+W)).$$\n",
    "So the gradient is $2A+W$.\n",
    "\n",
    "Solution 2: We can expand $f(A+dA)$ as follows:\n",
    "\\begin{align}\n",
    "f(A+dA) &= trace((A+dA)^T(A+dA)) + trace(W^T(A+dA)) \\\\\n",
    "        &= trace(A^TA + (dA)^TA + A^TdA + (dA)^TdA) + trace(W^TA+W^TdA)\\\\\n",
    "        &= trace(A^TA) + trace(W^TA) + trace((dA)^TA + A^TdA + W^TdA) + trace((dA)^TdA) \\\\\n",
    "        &= f(A) + trace((2A + W)^TdA) + trace((dA)^TdA) \\\\\n",
    "\\end{align}\n",
    "and so \n",
    "$$\\boxed{\\nabla f = 2A+W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 12\n",
    "\n",
    "Explain the following: If $A$ is a two by two rotation through a positive acute angle $\\theta$, then $A$ can not have real eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "If the matrix $A$ is a rotation matrix then every vector gets rotated by $\\theta$. If $A$ had real eigenvalues, then the eigenvalue equation $Ax=\\lambda x$ implies that any vector in the direction of $x$ will have its direction preserved - multiplying by $A$ would just scale it by the eigenvalue. But this is inconsistent with every vector being rotated. Therefore $A$ cannot have real eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 13\n",
    "\n",
    "Compute $d(A^3)$ in terms of $A$ and $dA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "Solution 1:  From the product rule , $d(A^3) = Ad(A^2) + dA A^2$.  Using the product rule\n",
    "again, we have $d(A^2)=AdA + dA A$.  Putting both together gives \n",
    "$$\\boxed{d(A^3) = (dA)A^2+A(dA)A + A^2(dA)}$$.\n",
    "(More conveniently one can apply directly the product rule in the form\n",
    "$d(A*A*A)=dA*A *A + A*dA*A + A*A*dA$, which extends to any number of multiplicands).\n",
    "\n",
    "\n",
    "Solution 2: Let $f(A) = A^3$. Then \n",
    "\\begin{align}\n",
    "f(A+dA) &= (A+dA)^3\\\\\n",
    "        &= A^3+((dA)A^2+A(dA)A + A^2(dA)) + \\text{terms involving (dA)^2 or higher}\n",
    "\\end{align}\n",
    "and so \n",
    "$$\\boxed{d(A^3) = (dA)A^2+A(dA)A + A^2(dA)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 14\n",
    "\n",
    "What is the eigenvector with negative eigenvalue of the reflection matrix $I-2xx^T/x^Tx$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "To figure this out it's a good idea to first think about what a reflection matrix does. Everything orthogonal to the vector $x$ is unchanged by multiplying by the reflection matrix, while everything in the direction of $x$ gets multiplied by $-1$ - so $x$ is an eigenvector with eigenvalue $-1$. \n",
    "\n",
    "To see this symbolically, lets multiply $R = I-2xx^T/x^Tx$ by $x$\n",
    "$$(I-2xx^T/x^Tx)x = x - 2x(x^Tx)/x^Tx = x-2x = -x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 15\n",
    "\n",
    "What is the trace of a rank one projection matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $P$ be a rank one projection matrix. The only possible eigenvalues for a projection matrix are $0$ and $1$. If $P$ is rank 1, then it has a $n-1$ dimensional nullspace, and so must have $n-1$ zero eigenvalues; and therefore exactly one eigenvalue equal to 1. Since the trace of a matrix is the sum of the eigenvalues, the trace of $P$ is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 16\n",
    "\n",
    "The number of nonzero eigenvalues of a symmetric matrix is always (a) greather than or equal to the rank (b) less than or equal to the rank (c) equal to the rank.  Pick the best answer and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Every symmetric matrix is diagonalizable, therefore every eigenvalue has a distinct eigenvector (even if the eigenvalues are repeated). There will be $n$ eigenvectors - each zero eigenvalue will therefore correspond to a distinct vector in the (n-r) dimensional nullspace, and each nonzer oeigenvalue will correspond to a distinct vector in the r dimensional column space. Therefore the number of nonzero eigenvalues is always exactly equal to the rank of a symmetric matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 17\n",
    "\n",
    "A markov matrix has second eigenvalue $\\lambda_2 = \\left(\\frac{1}{1000}\\right)^{\\frac{1}{1806}}$ and all other eigenvalues much smaller. After 1806 transition steps, approximately how many digits of the steady state might you expect to see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "For a Markov matrix $M$ and any vector $v$, we can write\n",
    "\\begin{align}\n",
    "M^nv &= c_1\\lambda_1^n v_1 + c_2\\lambda_2^n v_2 + ...\\\\\n",
    "&= c_1 v_1 + c_2\\left(\\frac{1}{1000}\\right)^{\\frac{n}{1806}} v_2 + ...\n",
    "\\end{align}\n",
    "So after $n=1806$ transition steps we have\n",
    "\\begin{align}\n",
    "M^{1806}v = c_1 v_1 + c_2\\left(\\frac{1}{1000}\\right) v_2 + ...\n",
    "\\end{align}\n",
    "Assuming the components of $v_1$ and $v_2$ and the constants $c_1$ and $c_2$ are approximately the same size, we would obtain the first three digits of the steady state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 18\n",
    "\n",
    "A symmetric matrix has eigenvalues $1,2,3,4,5$.  Give a direct argument (without using the 7 equivalent properties of positive definite matrices) that $x^T Sx>0$ if $x$ is not $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Since $S$ is symmetric, we know that it is diagonalizable we can write $S = Q\\Sigma Q^T$, where $Q$ is an orthogonal matrix. Since all the eigenvalues are positive, we can write $\\Sigma = M^2$, where $M$ is a diagonal matrix whose elements are the square roots of the eigenvalues. Then\n",
    "$$x^TSx = x^TQM^2Q^Tx = (MQ^Tx)^T(MQ^Tx) \\geq 0$$\n",
    "Since $Q$ is square orthogonal (and therefore full rank), $Q^Tx$ can only be zero when $x=0$. Similarly $M$ is diagonal with strictly positive entries on the diagonal, and so $MQ^Tx$ can only be zero when $x=0$. Therefore \n",
    "$$x^TSx = (MQ^Tx)^T(MQ^Tx) > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 19\n",
    "\n",
    "Find a diagonal similarity transformation that proves that the matrix $\\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}$  is similar to $\\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "One can look for a $t$ such that\n",
    "$$\\begin{pmatrix} 1 & 0 \\\\ 0 & 1/t \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 0 \\\\ 0 & t \\end{pmatrix} =\n",
    "\\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix}.$$\n",
    "It is readily seen that $t=2$ so\n",
    "$$\\begin{pmatrix} 1 & 0 \\\\ 0 & 1/2 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} =\n",
    "\\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 20\n",
    "\n",
    "Suppose that $A = QBQ^T$ where $Q$ is orthogonal but $A$ and $B$ need not be symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20a)  Are $A$ and $B$ necessarily similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20b) Do $A$ and $B$ necessarily have the same singular values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(a) $A$ and $B$ are similar whenever you have $A=XBX^{-1}$ for some matrix $X$. In this case $X=Q$ is orthogonal, and so $X^{-1}=Q^T$.\n",
    "\n",
    "(b) $A$ and $B$ necessarily have the same singular values. Suppose $B$ has a svd $B = U\\Sigma V^T$. Then\n",
    "$$A = QBQ^T = QU\\Sigma V^TQ^T = (QU)\\Sigma(QV)^T$$\n",
    "which is in svd form. So $A$ and $B$ have the same singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 21\n",
    "\n",
    "If $A$ is positive definite and $X$ is non-singular, is $X^TAX$ necessarily positive definite?  Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "If $A$ is positive definite then $x^TAx >0$ for any $x\\neq 0$. Now consider \n",
    "$$x^TX^TAXx = (Xx)^T A (Xx) = y^T A y \\geq 0 $$\n",
    "for all $y$, where $y = Xx$. However, since $X$ is non-singular, $y=0$ if and only if $x=0$. And so \n",
    "$$x^TX^TAXx >0$$\n",
    "for all $x\\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 22\n",
    "\n",
    "Are the matrices similar to positive Markov matrices necessarily positive Markov?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "No. Consider the matrix\n",
    "$$M = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}.$$\n",
    "This is a positive Markov matrix with eigenvalues $1$ and $0$. Hence this matrix is similar to the diagonal matrix\n",
    "$$D = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},$$\n",
    "which is certainly not positive Markov, since three of its entries are equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
