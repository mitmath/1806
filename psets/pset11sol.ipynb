{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 11 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & -5 & 6 \\\\ 3 & -2 & 5 \\\\ -4 & -4 & 0 \\end{pmatrix}\n",
    "$$\n",
    "The eigenvalues of $A$ are $0$ and $\\frac{-1 \\pm i\\sqrt{227}}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Complex{Float64},1}:\n",
       "        -0.5+7.53326im\n",
       "        -0.5-7.53326im\n",
       " 1.73659e-16+0.0im    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [ 1  -5  6\n",
    "      3  -2  5\n",
    "     -4  -4  0 ]\n",
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** The following is a plot of the solution $x(t)$ of\n",
    "$$\n",
    "\\frac{dx}{dt} = Ax\n",
    "$$\n",
    "for an initial condition $x(0) = (1,0,0)$.  Explain your solution in terms of the eigenvalues given above:\n",
    "\n",
    "* The solution approaches a constant vector because ........\n",
    "* As it approaches this constant, the solution oscillates with a period = ........"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Similar to your [solution to problem 4b of pset 10](http://nbviewer.jupyter.org/github/stevengj/1806/blob/fall18/psets/pset10sol.ipynb), the scalar $v^T x(t)$ is a *constant* (a \"conserved\" quantity) where the vector $v$ is in the ...... space of $A$, which is ....-dimensional for the matrix $A$ above.  $v$ is also an eigenvector of the matrix .......?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "**(a)** The solution to this ODE can be written\n",
    "\\begin{align}\n",
    "x(t) = c_1e^{\\lambda_1 t} v_1 + c_2e^{\\lambda_2 t} v_2 + c_3e^{\\lambda_3 t} v_3\n",
    "\\end{align}\n",
    "where $\\lambda_1 = 0, \\lambda_{2,3} = \\frac{-1\\pm i\\sqrt{227}}{2}$ and $v_1,v_2,v_3$ are the corresponding eigenvectors. \n",
    "\n",
    "Since the real parts of $\\lambda_2$ and $\\lambda_3$ are $<0$, this part of the solution will decay. So the long time dynamics are dictated by the part of the solution corresponding to $\\lambda_1 = 0$, and so as $t\\to\\infty$, $x \\to c_1v_1$. \n",
    "\n",
    "As it approaches this constant, the solution oscillates because of the imaginary parts of $\\lambda_2$ and $\\lambda_3$. The frequency of these oscillations is dictated by the imaginary part of the complex eigenvalue pair, i.e. $\\omega = \\frac{\\sqrt{227}}{2}$ and so the period of these oscillations is $\\frac{2\\pi}{\\omega} = \\frac{4\\pi}{\\sqrt{227}}$.\n",
    "\n",
    "**(b)** The scalar $v^Tx(t)$ is conserved if $\\frac{d}{dt}\\left(v^Tx(t)\\right) = 0$, or equivalently:\n",
    "\\begin{align}\n",
    "\\boxed{v^TAx = 0 \\;\\; \\text{for all} \\;\\; x}\n",
    "\\end{align}\n",
    "This means that $v$ must be in the **left nullspace** of $A$. $A$ has a single zero eigenvalue, and so the rank of $A$ is 2. This means the left nullspace of $A$ is **one-dimensional**. Since $v^TA = 0 \\iff A^Tv=0$, $v$ is also an eigenvector of the matrix $A^T$ with eigenvalue $\\lambda=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "(Based on Strang, section 6.3, problem 31.)\n",
    "\n",
    "The **sine of a matrix** can be defined similarly to $e^A$, by copying the [Taylor series for](https://en.wikipedia.org/wiki/Trigonometric_functions#Series_definitions) $\\sin t$:\n",
    "\n",
    "$$\n",
    "\\sin A = A - \\frac{A^3}{3!} + \\frac{A^5}{5!} - \\frac{A^7}{7!} + \\cdots .\n",
    "$$\n",
    "\n",
    "**(a)** If $Ax = \\lambda x$, multiply each term in the series by $x$ to find an eigenvalue of $\\sin A$.\n",
    "\n",
    "**(b)** Explain, using the series, why $\\frac{d^2}{dt^2} \\sin(At) = -A^2 \\sin(At)$.\n",
    "\n",
    "**(c)** Find a solution of the form $u(t) = \\sin(At) w$, where $w$ is some vector, to $\\frac{d^2 u}{dt^2} = -A^2 u$, given the initial conditions $u(0) = 0$ and $\\left . \\frac{du}{dt} \\right|_{t=0} = v_0$, assuming $A$ is invertible.\n",
    "\n",
    "**(d)** If $A = \\begin{pmatrix} \\pi/2 & \\pi/2 \\\\ \\pi/2 & \\pi/2 \\end{pmatrix}$, it has eigenvectors $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.   Find the corresponding eigenvalues, and use them to compute the matrix $\\sin A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** If $Ax=\\lambda x$, then \n",
    "\\begin{align}\n",
    "\\left(\\sin{A}\\right) x &= \\left(A - \\frac{A^3}{3!} + \\frac{A^5}{5!} - \\frac{A^7}{7!} + \\cdots\\right)x\\\\\n",
    "&= Ax - \\frac{A^3x}{3!} + \\frac{A^5x}{5!} - \\frac{A^7x}{7!} + \\cdots\\\\\n",
    "&= \\left(\\lambda - \\frac{\\lambda^3}{3!} + \\frac{\\lambda^5}{5!} - \\frac{\\lambda^7}{7!} + \\cdots\\right)x\\\\\n",
    "&= (\\sin{\\lambda}) x\n",
    "\\end{align}\n",
    "So $x$ is an eigenvector of $\\sin{A}$ with eigenvalue $\\sin{\\lambda}$. \n",
    "\n",
    "**(b)** We can differentiate the series with respect to $t$:\n",
    "\\begin{align}\n",
    "\\frac{d^2}{dt^2} \\sin(At) &= \\frac{d^2}{dt^2}\\left((At) - \\frac{(At)^3}{3!} + \\frac{(At)^5}{5!} - \\frac{(At)^7}{7!} + \\cdots\\right)\\\\\n",
    "&= \\frac{d}{dt}\\left(A - \\frac{A^3t^2}{2!} + \\frac{A^5t^4}{4!} - \\frac{A^7t^6}{6!} + \\cdots\\right)\\\\\n",
    "&= \\left( - \\frac{A^3t}{1!} + \\frac{A^5t^3}{3!} - \\frac{ A^7t^5}{5!} + \\cdots\\right)\\\\\n",
    "&= -A^2\\left((At) - \\frac{(At)^3}{3!} + \\frac{(At)^5}{5!} + \\cdots\\right)\\\\\n",
    "&= -A^2\\sin(At) \n",
    "\\end{align}\n",
    "\n",
    "**(c)** Since $\\frac{d^2}{dt^2} \\sin(At) = -A^2\\sin(At)$, then $u(t) = \\sin(At)w$ with $w$ constant is certainly a solution of the differential equation:\n",
    "\\begin{align}\n",
    "\\frac{d^2 u}{dt^2} = \\frac{d^2}{dt^2}\\left(\\sin(At)w\\right) = -A^2\\sin(At)w = -A^2 u.\n",
    "\\end{align}\n",
    "By definition, $u(0) = \\sin(0)w = 0$. In order to satisfy the second initial condition, we require $\\left . \\frac{du}{dt} \\right|_{t=0} = v_0$. By differentiating the series only once, we obtained \n",
    "\\begin{align}\n",
    "\\frac{d}{dt} \\sin(At) = A\\cos{At}\n",
    "\\end{align}\n",
    "and so  $\\left . \\frac{du}{dt} \\right|_{t=0} = \\left[ A\\cos(At) w \\right]_{t=0} = Aw$. In order to enforce this second condition we then require that $Aw = v_0$. We assume $A$ is invertible, and so there is a unique $w$ given by:\n",
    "\\begin{align}\n",
    "\\boxed{w = A^{-1}v_0}.\n",
    "\\end{align}\n",
    "\n",
    "**(d)** We firstly multiply $A$ by $v_1$ and $v_2$ and find:\n",
    "\\begin{align}\n",
    "Av_1 = \\pi v_1, \\;\\; Av_2 = 0.\n",
    "\\end{align}\n",
    "So $A$ has eigenvalues $0$ and $\\pi$. This means $\\sin(A)$ has a repeated eigenvalue of $0$, with the same eigenvectors as $A$. However, a $2\\times 2$ matrix with two eigenvalues of $0$ with two independent eigenectors will have rank $0$. The only matrix with rank $0$ is the zero matrix, and so\n",
    "\\begin{align}\n",
    "\\sin(A) = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "If $A$ is a real matrix with $A = -A^T$, then\n",
    "\n",
    "**(a)** Explain why $U = e^A$ is an orthogonal matrix.   Check this in Julia by constructing a random $A$ with `A = rand(4,4); A = A - A'`, exponentiating with `U = expm(A)`, and checking that `U` is orthogonal (up to roundoff errors).\n",
    "\n",
    "**(b)** If $x(t)$ satisfies $$\\frac{dx}{dt} = Ax$$ then explain why your answer from (a) implies that $\\Vert x(t) \\Vert = \\Vert x(0) \\Vert$ for all $t$.\n",
    "\n",
    "**(c)** The matrix $iA$ is a ............ matrix with eigenvalues that are purely ......., so $A$ must have eigenvalues that are purely ........ and eigenvectors that are .......   Check this with `eigvals(A)` for your matrix from (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "**(a)** $U=e^A$ is square.  A square matrix $U$ is orthogonal if $U^TU = I$. If $U = e^A$, then $U^T = e^{(A^T)}$:\n",
    "$$\n",
    "(e^A)^T = \\left( I + A + \\frac{A^2}{2!} + \\cdots\\right)^T = I + A^T + \\frac{(A^T)^2}{2!} + \\cdots = e^{(A^T)} \\, .\n",
    "$$\n",
    "But $A^T = -A$, and so $U^T = e^{-A} = [e^A]^{-1}$, and so $U$ is an orthogonal matrix. We can check this numerically in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "  1.0          -1.40047e-17  1.04767e-16  -1.30799e-17\n",
       " -1.40047e-17   1.0          9.62867e-17  -3.80949e-18\n",
       "  1.04767e-16   9.62867e-17  1.0           2.30669e-17\n",
       " -1.30799e-17  -3.80949e-18  2.30669e-17   1.0        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(4,4); A = A - A'\n",
    "U = expm(A)\n",
    "U'U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see then that $U^TU=I$ up to roundoff error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** If $x(t)$ satisfies $$\\frac{dx}{dt} = Ax,$$ then we know that the exact solution may be written as $$x(t) = e^{At} x(0).$$ Then $$\\Vert x(t) \\Vert^2 = x(t)^T x(t) = x(0)^T [e^{At}]^Te^{At} x(0) = x(0)^Tx(0) = \\Vert x(0) \\Vert^2.$$ And so $\\Vert x(t) \\Vert = \\Vert x(0) \\Vert$ for all $t$.\n",
    "\n",
    "(You may have forgotten this, but showed a similar thing in class a long time ago: $\\Vert Qx \\Vert^2 = x^T Q^T Q x = \\Vert x \\Vert^2$ for any matrix $Q$ with orthonormal columns.  Thus, since $Q = e^{At}$ is an orthogonal matrix from part (a), the norm is preserved by time evolution!)\n",
    "\n",
    "**(c)** The matrix $iA = (iA)^H = -iA^T$ is a **Hermitian** matrix with eigenvalues that are purely **real**, so $A$ must have eigenvalues that are purely **imaginary** and eigenvectors that are **orthogonal**. We can then verify this by calculating the eigenvalues of our matrix from (a) in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Complex{Float64},1}:\n",
       " -1.04083e-17+1.11098im \n",
       " -1.04083e-17-1.11098im \n",
       " -6.93889e-18+0.226511im\n",
       " -6.93889e-18-0.226511im"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "You are given the matrix\n",
    "$$\n",
    "A = \\begin{pmatrix} 7 & 4 & -5 \\\\ 4 & -2 & 4 \\\\ -5 & 4 & 7 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**(a)** Two of the eigenvalues of $A$ are 6 and –6.  Find the third eigenvalue.  The eigenvalues must be real because $A$ is .......   Check your answer in Julia using `eigvals(A)`.\n",
    "\n",
    "**(b)** Two eigenvectors of $A$ are the column vectors (1,1,1) and (1,-2,1).  Find the third eigenvector.  The eigenvectors must be ....... to one another because $A$ is .......\n",
    "\n",
    "**(c)** Suppose that $x(t)$ is the solution to $\\frac{dx}{dt} = Ax - 12x$ with $x(0) = (1,0,0)$.   After a long time $x(t)$ is approximately parallel to what vector?   Check your answer by computing $x(t) = e^{At} x(0)$ in Julia for a large $t$ using `expm((A-12I)*t)`.\n",
    "\n",
    "**(d)** Give an exact expression for $x(t)$ from (c).   You should be able to write your answer easily, without doing Gaussian elimination or anything fancy — only a few dot products need to be computed, because the eigenvectors are ..........   Check your answer in Julia using `expm((A-12I)*t)`.\n",
    "\n",
    "**(e)** As $t \\to \\infty$, the matrix $e^{(A-12I)t}$ approaches what projection matrix $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "**(a)** The trace of a matrix is the sum of the eigenvalues. We firstly compute the trace of $A$ by summing its diagonal values $7+(-2)+7 = 12$. Since $6+(-6)=0$, the third eigenvalue is then necessarily $\\lambda_3 = 12$. The eigenvalues must be real because $A$ is **real-symmetric**. We can check our answer in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -6.0\n",
       "  6.0\n",
       " 12.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [ 7  4 -5\n",
    "      4 -2  4\n",
    "     -5  4  7 ]\n",
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed the matrix has eigenvalues $6,-6$ and $12$.\n",
    "\n",
    "**(b)** Multiplying $A$ by $v_1 = (1,1,1)$ and by $v_2 = (1,-2,1)$, we can see that $v_1$ is the eigenvector corresponding to $\\lambda_1 = 6$ and $v_2$ is the eigenvector corresponding to $\\lambda_2=-6$. \n",
    "\n",
    "The most direct way to find $v_3$ is to find a solution of $(A-12I)v_3 = 0$, which implies:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} -5 & 4 & -5 \\\\ 4 & -14 & 4 \\\\ -5 & 4 & -5 \\end{pmatrix}v_3 = 0\n",
    "\\end{align}\n",
    "\n",
    "A possible eigenvector is then $$v_3 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}.$$\n",
    "\n",
    "The eigenvectors must be **orthogonal** to one another because $A$ is **real-symmetric**.   This gives us *another way* to find $v_3$: we just need a vector orthogonal to $v_1$ and $v_2$.   We could do this by inspection and see that we must have a vector of the form $(a,0,-a)$, or we could take the cross product $v_1 \\times v_2 = (3,0,-3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The eigenvalues of (A-12I) are $-6$, $-18$ and $0$, with the same corresponding eigenvectors as $A$. \n",
    "\n",
    "This means that the general solution to $\\frac{dx}{dt} = Ax - 12x$ can be written as $$x(t) = c_1 e^{-6t}v_1 + c_2 e^{-18 t}v_2 + c_3 v_3.$$ \n",
    "\n",
    "As $t$ becomes very large, the first two terms in the solution will decay and so the solution will become parallel to the third eigenvector $$v_3 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}.$$ We can compute the solution for large time t using Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.5         \n",
       "  8.83466e-262\n",
       " -0.5         "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = expm((A-12I)*100)*[1;0;0] #using t=100 as 'large time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed the solution becomes parallel to $v_3 $ as $t\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** We can find the constants $c_1, c_2$ and $c_3$ easily since the eigenvectors are **orthogonal**. We require \n",
    "\\begin{align}\n",
    "x(0) = c_1v_1 + c_2 v_2 + c_3 v_3 = \\begin{pmatrix} 1\\\\0\\\\0\\end{pmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "So we can take the dot product of both sides with each of the eigenvectors in turn in order to compute the constants:\n",
    "\\begin{align}\n",
    "c_1 &= \\frac{v_1^T (1,0,0)}{v_1^Tv_1} = \\frac{1}{3}\\\\\n",
    "c_2 &= \\frac{v_2^T (1,0,0)}{v_2^Tv_2} = \\frac{1}{6}\\\\\n",
    "c_3 &= \\frac{v_3^T (1,0,0)}{v_3^Tv_3} = \\frac{1}{2}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The exact solution is then \n",
    "\\begin{align}\n",
    "x(t) &= \\frac{1}{3} e^{-6t}v_1 + \\frac{1}{6} e^{-18 t}v_2 + \\frac{1}{2} v_3\\\\\n",
    "&= \\begin{pmatrix} \\frac{1}{3}e^{-6t} + \\frac{1}{6} e^{-18 t} + \\frac{1}{2} \\\\\n",
    "                   \\frac{1}{3}e^{-6t} - \\frac{1}{3} e^{-18 t} \\\\\n",
    "                   \\frac{1}{3}e^{-6t} + \\frac{1}{6} e^{-18 t} - \\frac{1}{2} \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.710487\n",
       "  0.127838\n",
       " -0.289513"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=0.1;\n",
    "expm((A-12I)*t)*[1;0;0] #Check for t = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.710487\n",
       "  0.127838\n",
       " -0.289513"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/3*exp(-6*t)*[1;1;1] + 1/6*exp(-18*t)*[1;-2;1] + 1/2*[1;0;-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so our solution correctly computes $x(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** As $t \\to \\infty$, the matrix $$e^{(A-12I)t} = Q\\begin{pmatrix} e^{-6t} & & \\\\ & e^{-18t} & \\\\ & & 1\\end{pmatrix}Q^T$$ (where $Q$ is the matrix of eigenvectors normalized to length 1) will approach the rank one matrix whose column space is spanned by $v_3$, i.e. \n",
    "\\begin{align}\n",
    "e^{(A-12I)t} \\to \\frac{v_3 v_3^T}{v_3^T v_3} = \\frac{1}{2}\\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "In fact, this is exactly the **orthogonal projection** matrix onto the line spanned by $v_3$, i.e. onto $N(A-12I)$.\n",
    "\n",
    "Double-checking in Julia, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  0.5  0.0  -0.5\n",
       "  0.0  0.0   0.0\n",
       " -0.5  0.0   0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expm((A-12I)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which matches!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
